# NFL Penalty Data Machine Learning Project

## Project Overview

This project focuses on the machine learning analysis of NFL penalties to predict their occurrence and evaluate their effectiveness in terms of drive outcomes. Through comprehensive data collection, cleaning, and analysis, we aim to provide insights into how penalties influence the game's outcome and offer predictive models that could be useful for teams, analysts, and fans alike.

Our final report can be found at `docs/final-report.pdf` along with our in class presentation slides at `docs/ML Final Presentation.pdf`. Our models and EDA notebooks can be found in the `notebooks/` directory.

## Data Collection

The data collection process involved writing custom scripts to scrape NFL game and penalty data from various sources. This process was divided into several steps:

- **Identifying Missing Data (`missing.py`):** This script checks for any missing game data by comparing our existing dataset with the list of all NFL games within our study's timeframe. It generates a list of URLs (games) that need to be scraped.
- **Scraping Game Data (`scrape_games.py`):** Utilizing the list generated by `missing.py`, this script scrapes detailed game data from specified URLs. It collects information critical for our analysis, including game outcomes, teams, dates, and more.
- **Scraping Penalty Data (`scrape_penalties.py`):** This script is dedicated to extracting detailed information on penalties from each game. It collects data such as penalty type, penalized team, game impact, and more.

## Data Cleaning and Schema Finalization

The scraped data is then cleaned to a relational schema of csv files. These files have corresponding fields of game_id and team_id, allowing for relational analysis. 

- **Data Cleaning:** Addressing missing values, removing outliers, and standardizing formats across the dataset.
- **Schema Design:** Finalizing a database schema that supports efficient analysis and machine learning model development. This included defining the structure for storing games, penalties, teams, and their relationships.

## Exploratory Data Analysis

The next step that was implemented was the exploratory data analysis of the `penalties.csv` file. The following graphs were created for a better understanding of the dataset, and can be found at `penalties_eda.ipynb`:

- Most Common Offensive Penalties
- Most Common Defensive Penalties
- Most Yards by Offensive Penalties
- Most yards by Defensive Penalties
- Most Yards Lost by Teams
- Most Yards Gained by Teams
- Most Yards by Position
- Average Number of Penalties by Ref Crew
- Penalty Counts by Minute (Timeseries)

## Model Descriptions

We used a variety of models to uncover predictive insights on the amount of each type of penalty for a given game, and the amount of points a drive will result in.

The models we used are as follows:

- In `drives.ipynb`, we implemented a GradientBoostingClassifier and a GradientBoostingRegressor model, along with a prediction function. These models predict the end result of a drive (Touchdown, Field Goal, or Nothing), taking in [`total_off_pen`, `total_def_pen`, `total_off_pen_yards`, `total_def_pen_yards`, `los`, `time_left_seconds`] as input features. We found the Classifier model to be the best performing model, as the introduction of the respective points for each result negatively impacted the Regressor model.

- In `penalties.ipynb`, we implemented an ensemble of NegativeBinomial models, along with a prediction function. These models predict the amount of penalties for each type of penalty, taking in [`team_id`, `opp_id`, `year`, `week`, `ref_crew`, `home`, `postseason`]  as input features. Unfortunately we found that no matter what type of model we used, the end result always came up to be pretty much the same as a regular statistical average.

- In `nn.ipynb`, we attempted to fix the underyling issues found in our NegativeBinomial ensemble model by creating a custom neural network with TensorFlow and Keras. Our neural network implementation ended up with extremely similiar results to our intial model, that we concluded the lack of singular player data in our schema severely constricts predictive performance.

## How to Use This Project

### Scraping
To collect the data needed for analysis, follow these steps:
1. **Run `missing.py`:** Identifies the games that need data collection.
2. **Execute `scrape_games.py`:** Collects game data based on the output from `missing.py`.
3. **Run `scrape_penalties.py`:** Collects penalty data and is not related to `missing.py`.

### Cleaning
To clean the data to our schema:
1. **Run `clean_penalties.py`:** Cleans the penalties.csv file to the processed directory.
2. **Run `clean_drives.py`:** Cleans the drives.csv file to the processed directory.
3. **Run `clean_games.py`:** Cleans the team_performances.csv file to the processed directory.

### Model Creation and EDA
Notebooks should run as intended once the dependent libraries are installed.

## Dependencies

This project requires the following libraries:

### Scraping Aspect
- BeautifulSoup
- Selenium
- pandas
- requests

You can install these dependencies using pip:

```bash
pip install beautifulsoup4 selenium pandas requests
```

Additionally, you will need the appropriate ChromeDriver for Selenium.

### Model Aspect
- pandas
- matplotlib
- seaborn
- tensorflow
- keras
- keras_tuner
- sklearn
- statsmodels.api

```bash
pip install pandas matplotlib seaborn tensorflow keras keras_tuner sklearn statsmodels.api
```

## Credits

- **Eric:** Data scraping, cleaning, and model creation.
- **Leo:** Data cleaning, eda, and model creation.
